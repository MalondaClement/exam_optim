\documentclass[12pt]{article}
\input{/Users/ClementMalonda/Documents/Licence/Fiche/lib/general}
\usepackage[frenchb]{babel}
% \usepackage[english]{babel}

\begin{document}
\title{Examen Optimisation}
\author{Clément \textsc{Malonda}}
% \date{}
\maketitle

\lstset{language=python}

\section*{Présentation du problème}

Optimiser la fonction suivante :
$$f(\omega) = 418,9829\times D - \sum_{i=1}^{D}\omega_i \times \sin(\sqrt{|\omega_i|})$$

où $D$ est la dimension du problème. Dans notre cas, nous allons utiliser $D = 1$, $D = 2$ et $D = 10$, avec respectivement
\begin{itemize}
    \item[\textbullet] $\omega = 20$,
    \item[\textbullet] $\omega = [-20, 20]$
    \item[\textbullet]et $\omega = [20, 20, -20, 20, -20, 20, 20, 20, 20, 20]$.
\end{itemize}

Pour l'ensemble du sujet, j'ai fait le choix d'utiliser l'algorithme de descente de gradient avec momentum.

\section{Implémentation de la fonction et de son gradient}
\begin{lstlisting}
def f(w):
    res = 418.9829 * w.shape[0]
    sum = 0
    for i in range(w.shape[0]):
        sum = sum + w[i] * np.sin(np.sqrt(np.abs(w[i])))
    return res - sum
\end{lstlisting}

\begin{lstlisting}
def grad(w):
    res = 0
    for i in range(w.shape[0]):
        res = res + (w[i]**2 + np.cos(np.sqrt(np.abs(w[i])))) / (2*np.abs(w[i])**(3/2)) + np.sin(np.sqrt(np.abs(w[i])))
    return res
\end{lstlisting}

\section*{Problème en dimension 1}
\begin{lstlisting}
w0 = np.array([20])
lr = 0.7
res_1 = gd2_momentum(w0, grad, lr, max_iter=33)
\end{lstlisting}

\imgLarge{img/Figure_1.png}{Solution pour un problème de dimension 1}

Dans le cas où $D=1$ la solution arrive en 33 itération avec un learning rate fixé à $0,7$. La figure \ref{img:img/Figure_1.png} montre le profil de la fonction $f$ en bleu et les étapes de la solutions en rouge. La solution finale est $w = -0.04144376$.

\newpage
\section*{Problème en dimension 2}

\begin{lstlisting}
w0 = np.array([-20, 20])
lr = 0.6
res_2 = gd2_momentum(w0, grad, lr, max_iter=22)
\end{lstlisting}

\imgLarge{img/Figure_2.png}{Solution pour un problème de dimension 2}

Dans le cas où $D=2$ la solution semble diverger avec learning rate fixé à $0,7$ et 22 itérations. La figure \ref{img:img/Figure_2.png} montre le profil de la fonction $f$ sous forme de courbe de niveau et les étapes de la solutions en rouge. La solution finale est $w = [-39.1737414, 0.8262586]$.

Nous pouvons constater que la solution diverge, la valeur $\omega_2$ semble tendre vers 0 mais $\omega_2$ non.

Dans le cas d'un point de départ en $\omega = [-20, 20]$ alors on converge en 23 itération avec le même learning rate (cf. Figure \ref{img:img/Figure_2bis.png}).

\imgLarge{img/Figure_2bis.png}{Autre test pour le problème de dimension 2}

\newpage
\section*{Problème en dimension 10}
\begin{lstlisting}
w0 = np.array([20, 20, -20, 20, -20, 20, 20, 20, 20, 20])
lr = 0.1
res_2 = gd2_momentum(w0, grad, lr, max_iter=10)
\end{lstlisting}

w = [ 16.52988674, 16.52988674, -23.47011326, 16.52988674 -23.47011326
, 16.52988674, 16.52988674, 16.52988674, 16.52988674, 16.52988674]


\newpage
\section*{Fonction de gradient avec momentum}

\begin{lstlisting}
def gd2_momentum(x, grad, alpha, beta=0.9, max_iter=10):
    xs = np.zeros((1 + max_iter, x.shape[0]))
    xs[0, :] = x
    v = 0
    for i in range(max_iter):
        v = beta*v + (1-beta)*grad(x)
        vc = v/(1+beta**(i+1))
        x = x - alpha * vc
        xs[i+1, :] = x
    return xs
\end{lstlisting}


\end{document}
